{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import depthai as dai\n",
    "from time import sleep\n",
    "import datetime\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "datasetDefault = str((Path(__file__).parent / Path('models/dataset')).resolve().absolute())\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-dataset', nargs='?', help=\"Path to recorded frames\", default=datasetDefault)\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not Path(datasetDefault).exists():\n",
    "    import sys\n",
    "    raise FileNotFoundError(f'Required file/s not found, please run \"{sys.executable} install_requirements.py\"')\n",
    "\n",
    "\n",
    "# StereoDepth config options.\n",
    "out_depth = False  # Disparity by default\n",
    "out_rectified = True   # Output and display rectified streams\n",
    "lrcheck = True   # Better handling for occlusions\n",
    "extended = False  # Closer-in minimum depth, disparity range is doubled\n",
    "subpixel = True   # Better accuracy for longer distance, fractional disparity 32-levels\n",
    "median = dai.StereoDepthProperties.MedianFilter.KERNEL_7x7\n",
    "\n",
    "# Sanitize some incompatible options\n",
    "if lrcheck or extended or subpixel:\n",
    "    median = dai.StereoDepthProperties.MedianFilter.MEDIAN_OFF\n",
    "\n",
    "print(\"StereoDepth config options: \")\n",
    "print(\"Left-Right check: \", lrcheck)\n",
    "print(\"Extended disparity: \", extended)\n",
    "print(\"Subpixel: \", subpixel)\n",
    "print(\"Median filtering: \", median)\n",
    "\n",
    "right_intrinsic = [[860.0, 0.0, 640.0], [0.0, 860.0, 360.0], [0.0, 0.0, 1.0]]\n",
    "\n",
    "\n",
    "def create_stereo_depth_pipeline():\n",
    "    print(\"Creating Stereo Depth pipeline: \", end='')\n",
    "\n",
    "    print(\"XLINK IN -> STEREO -> XLINK OUT\")\n",
    "    pipeline = dai.Pipeline()\n",
    "\n",
    "    camLeft = pipeline.createXLinkIn()\n",
    "    camRight = pipeline.createXLinkIn()\n",
    "    stereo = pipeline.createStereoDepth()\n",
    "    xoutLeft = pipeline.createXLinkOut()\n",
    "    xoutRight = pipeline.createXLinkOut()\n",
    "    xoutDepth = pipeline.createXLinkOut()\n",
    "    xoutDisparity = pipeline.createXLinkOut()\n",
    "    xoutRectifLeft = pipeline.createXLinkOut()\n",
    "    xoutRectifRight = pipeline.createXLinkOut()\n",
    "\n",
    "    camLeft.setStreamName('in_left')\n",
    "    camRight.setStreamName('in_right')\n",
    "\n",
    "    stereo.setConfidenceThreshold(200)\n",
    "    stereo.setRectifyEdgeFillColor(0) # Black, to better see the cutout\n",
    "    stereo.setMedianFilter(median) # KERNEL_7x7 default\n",
    "    stereo.setLeftRightCheck(lrcheck)\n",
    "    stereo.setExtendedDisparity(extended)\n",
    "    stereo.setSubpixel(subpixel)\n",
    "\n",
    "    stereo.setEmptyCalibration() # Set if the input frames are already rectified\n",
    "    stereo.setInputResolution(1280, 720)\n",
    "\n",
    "    xoutLeft.setStreamName('left')\n",
    "    xoutRight.setStreamName('right')\n",
    "    xoutDepth.setStreamName('depth')\n",
    "    xoutDisparity.setStreamName('disparity')\n",
    "    xoutRectifLeft.setStreamName('rectified_left')\n",
    "    xoutRectifRight.setStreamName('rectified_right')\n",
    "\n",
    "    camLeft.out.link(stereo.left)\n",
    "    camRight.out.link(stereo.right)\n",
    "    stereo.syncedLeft.link(xoutLeft.input)\n",
    "    stereo.syncedRight.link(xoutRight.input)\n",
    "    if out_depth:\n",
    "        stereo.depth.link(xoutDepth.input)\n",
    "    stereo.disparity.link(xoutDisparity.input)\n",
    "    if out_rectified:\n",
    "        stereo.rectifiedLeft.link(xoutRectifLeft.input)\n",
    "        stereo.rectifiedRight.link(xoutRectifRight.input)\n",
    "\n",
    "    streams = ['left', 'right']\n",
    "    if out_rectified:\n",
    "        streams.extend(['rectified_left', 'rectified_right'])\n",
    "    streams.extend(['disparity', 'depth'])\n",
    "\n",
    "    return pipeline, streams\n",
    "\n",
    "\n",
    "def convert_to_cv2_frame(name, image):\n",
    "    baseline = 75 #mm\n",
    "    focal = right_intrinsic[0][0]\n",
    "    max_disp = 96\n",
    "    disp_type = np.uint8\n",
    "    disp_levels = 1\n",
    "    if (extended):\n",
    "        max_disp *= 2\n",
    "    if (subpixel):\n",
    "        max_disp *= 32\n",
    "        disp_type = np.uint16\n",
    "        disp_levels = 32\n",
    "\n",
    "    data, w, h = image.getData(), image.getWidth(), image.getHeight()\n",
    "    if name == 'depth':\n",
    "        # this contains FP16 with (lrcheck or extended or subpixel)\n",
    "        frame = np.array(data).astype(np.uint8).view(np.uint16).reshape((h, w))\n",
    "    elif name == 'disparity':\n",
    "        disp = np.array(data).astype(np.uint8).view(disp_type).reshape((h, w))\n",
    "\n",
    "        # Compute depth from disparity\n",
    "        with np.errstate(divide='ignore'):\n",
    "            depth = (disp_levels * baseline * focal / disp).astype(np.uint16)\n",
    "\n",
    "        if 1: # Optionally, extend disparity range to better visualize it\n",
    "            frame = (disp * 255. / max_disp).astype(np.uint8)\n",
    "\n",
    "        if 1: # Optionally, apply a color map\n",
    "            frame = cv2.applyColorMap(frame, cv2.COLORMAP_HOT)\n",
    "\n",
    "    else: # mono streams / single channel\n",
    "        frame = np.array(data).reshape((h, w)).astype(np.uint8)\n",
    "        if name.startswith('rectified_'):\n",
    "            frame = cv2.flip(frame, 1)\n",
    "        if name == 'rectified_right':\n",
    "            last_rectif_right = frame\n",
    "    return frame\n",
    "\n",
    "\n",
    "pipeline, streams = create_stereo_depth_pipeline()\n",
    "\n",
    "print(\"Connecting and starting the pipeline\")\n",
    "# Connect and start the pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    inStreams = ['in_right', 'in_left']\n",
    "    inStreamsCameraID = [dai.CameraBoardSocket.RIGHT, dai.CameraBoardSocket.LEFT]\n",
    "    in_q_list = []\n",
    "    for s in inStreams:\n",
    "        q = device.getInputQueue(s)\n",
    "        in_q_list.append(q)\n",
    "\n",
    "    # Create a receive queue for each stream\n",
    "    q_list = []\n",
    "    for s in streams:\n",
    "        q = device.getOutputQueue(s, 8, blocking=False)\n",
    "        q_list.append(q)\n",
    "\n",
    "    # Need to set a timestamp for input frames, for the sync stage in Stereo node\n",
    "    timestamp_ms = 0\n",
    "    index = 0\n",
    "    while True:\n",
    "        # Handle input streams, if any\n",
    "        if in_q_list:\n",
    "            dataset_size = 2  # Number of image pairs\n",
    "            frame_interval_ms = 500\n",
    "            for i, q in enumerate(in_q_list):\n",
    "                path = args.dataset + '/' + str(index) + '/' + q.getName() + '.png'\n",
    "                data = cv2.imread(path, cv2.IMREAD_GRAYSCALE).reshape(720*1280)\n",
    "                tstamp = datetime.timedelta(seconds = timestamp_ms // 1000,\n",
    "                                            milliseconds = timestamp_ms % 1000)\n",
    "                img = dai.ImgFrame()\n",
    "                img.setData(data)\n",
    "                img.setTimestamp(tstamp)\n",
    "                img.setInstanceNum(inStreamsCameraID[i])\n",
    "                img.setType(dai.ImgFrame.Type.RAW8)\n",
    "                img.setWidth(1280)\n",
    "                img.setHeight(720)\n",
    "                q.send(img)\n",
    "                if timestamp_ms == 0:  # Send twice for first iteration\n",
    "                    q.send(img)\n",
    "                print(\"Sent frame: {:25s}\".format(path), 'timestamp_ms:', timestamp_ms)\n",
    "            timestamp_ms += frame_interval_ms\n",
    "            index = (index + 1) % dataset_size\n",
    "            sleep(frame_interval_ms / 1000)\n",
    "        # Handle output streams\n",
    "        for q in q_list:\n",
    "            if q.getName() in ['left', 'right', 'depth']: continue\n",
    "            frame = convert_to_cv2_frame(q.getName(), q.get())\n",
    "            cv2.imshow(q.getName(), frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
